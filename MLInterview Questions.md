# Machine Learning Interview Questions

## Cross Entropy or Log Loss

Cross-entropy is commonly used to quantify the difference between two probability distributions.</br>
Cross-entropy loss measures how close is the predicted distribution to the true distribution.

![](https://github.com/theainerd/MLInterview/blob/master/images/Screenshot%20from%202018-08-12%2009-28-06.png)

Why the Negative Sign? </br>
Log Loss uses negative log to provide an easy metric for comparison. It takes this approach because the positive log of numbers < 1 returns negative values, which is confusing to work with when comparing the performance of two models.

